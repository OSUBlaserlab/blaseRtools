---
title: "scRNA-seq Data Processing"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{scRNAseq}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

## Introduction

This vignette will show you how to load 10X single cell RNA seq data into R and to perform "vanilla" processing. Here is an overview of the steps we will cover here:

1.  Pre-processing data using 10X genomics cloud.
2.  Loading data into a list of cellDataSet objects.
3.  QC Functions
4.  Merging into a single cellDataSet object.
5.  Dimension reduction
6.  Batch correction
7.  Clustering
8.  Cell assignments using label transfer
9.  Gene modules
10. UMAP Plot Types
11. Gene Dotplots
12. Differential Gene Expression

I use these steps on almost every scRNA-seq analysis I perform. With proper configuration, steps 1-8 can mostly be run non-interactively. I usually will make a "dry run" interactively as I build the processing script. Then I will refresh the R session and source the script to run from beginning to end.

The last steps require some interactive input and may be customized depending on the analysis you are doing. These usually become small standalone scripts.

### An important note about data management

An important fact to note is that the way we run these algorithms (UMAP dimension reduction and clustering in particular), the results are subtly non-deterministic. By this I mean that there will be subtle or at least arbitrary changes in the way the clusters are defined or the exact dimensions of particular cells. This does not change the underlying expression data or affect the analysis negatively except that if you were to run the same script twice, cluster 5 may become cluster 6 and UMAP dimensions may look different. The bottom line is that it is important to know which steps are deterministic and which are not. Also, which steps are computationally intensive and which are not.

In order to deal with these issues, there are particular data objects which you will want to save as .rda files and then go back to if needed in the future. Usually this will be the final cellDataSet object and some qc files. Using these things you can quickly and reproducibly generate a large number of plots. I will point out the points where you want to save these objects in this vignette.

A further note is that I am in the practice of packaging all of the pre-computed data objects into formal R packages. This aids portability, prevents unintentional overwriting, lost data, manages versions...basically a lot of good things for rigorous reproducible science. However, building R packages is outside the scope of this vignette. I may create a vignette on this in the future, but in the mean time here is a very good source: <https://kbroman.org/pkg_primer/> This is where I learned 90% of how to build an R package.

Good luck!!

## Pre-processing data using 10X genomics cloud

As of 2021/2022, the preferred method of going from sequence data (FASTQs) to cell-barcode matrices is using the 10X genomics cloud: <https://www.10xgenomics.com/products/cloud-analysis>. The benefits over running this locally are too numerous to list. The only drawback is that it requires some time to upload your data and you have to do some point and click to get things configured. So anyways, if you are working with FASTQs, this is what you should do.

Using 10X genomics cloud is free for the most part. It only restricts the number of downloads you can perform for a given dataset. It does require some very basic familiarity with the unix/linux command-line interface, but it does provide premade commands for you to copy and paste into the terminal.

The 10X Cloud outputs a "pipestance". This is a standard data structure which you should always download as-is and not edit or add to once you have downloaded. Including BAM files and dimensionality reduction is optional and I recommend against it in most cases. Unless you are doing things like RNA velocity or spliceoform analysis you don't need the BAMs and they are very large. We will be doing our own dimensionality reduction and clustering so we don't need those files either.

The recommended approach is to download these files directly to high-capacity storage, usually on an institutional network drive. That way you don't have to worry about backup and size. Once the data is archived there you will read from it very few times so data transfer time is not really an issue. Most of your downstream work will be with compressed R objects which are small and portable. I usually keep the FASTQs and the pipestances for each project together. You'll want to make it very clear using either README files or with the file structure itself, which FASTQs generated which pipestances. It isn't scripted (the drawback I mentioned above) so if you want to know what you did a year from now, you have to find a way to make it completely obvious to yourself.

## Loading data into a list of cellDataSet objects

The first thing you have to do is load the 10X Data for each specimen into a list of cellDataSet objects. You will use a configuration file to do this. First some more detail on these topics:

-   10X Data: At minimum, you need 1 file which is a zipped tar file containing the cell barcodes, genes, and gene/barcode matrix.  Depending on the specific analysis pipeline run, it may be named "filtered_feature_bc_matrix.tar.gz", or "sample_feature_bc_matrix.tar.gz".  The difference depends whether you are running the "counts" pipeline or the "multi" pipeline.  You don't want the raw_feature_bc_matrix which contains droplets below the UMI cutoff (basically empty droplets) and un-de-multiplexed data if you are using multiplexing.  If you are using 5' with VDJ sequencing, it will automatically run it through the multi pipeline even if you aren't using multiplexing, so you have to navigate a bit deeper to find the per_sample_outs directory.  It isn't hard to find if you know what to look for.

    The other things you will most likely want but we won't cover in detail here are the metrics summary file and any VDJ sequencing if you are doing that.  It is easier just to look at the html metrics summary file but if you want to programmatically look at the sequencing metrics file in R you can read that in.  VDJ may be covered in another vignette.

-   cellDataSet: Abbreviated CDS, this is the main data structure for holding the single cell data. It is derived from the Bioconductor SingleCellExperiment class. It can be thought of as a database holding a table of cell metadata, a table of gene metadata, and matrix of expression data, all of which are built from the pipestance data you load in. There are additional "slots" available in the CDS class for reduced dimensions of various types and cluster assignments. These we don't interact with directly for the most part. I find the data structure very simple and straighforward to understand and work with. All of the scRNA-seq functions in this package operate on this data structure. I find the Seurat data structure less user-friendly. Where we need to use Seurat-only functions (like label transfer and Doubletfinder), Seurat objects are generated "behind the scenes", so you don't really need to know them well to use the blaseRtools functions. If you understood this paragraph then you already understand the CDS data structure which is half the battle.

-   configuration file: Abbreviated config, this is a text file (csv) that you can edit in excel and which will hold sample-level metadata, including the file path where the pipestance is stored. You can see an example I used for this vignette here: `system.file("extdata/vignette_config.csv", package = "blaseRdata")`.  At a minimum it should have a column named "sample" and a column named "targz_path" (the name for the latter is not critical).

-   sample: It is natural for most people to refer to the biological specimen that produced the data as a "sample".  However this term gets rather overloaded in these analyses, meaning that it gets used for too many things in too many different places.  If you want to be safer, you can call it "specimen" or something else.  If you want to use "sample" as I do here, you have to make sure it is not added explicitly as a cell metadata column.  If you do it will cause an error with a function that will try to create a "sample" column automatically.  Hopefully that will be more clear below.    

One final note:  since some of the data processing are time-consuming, most of the code listed here will not be executed with the vignette.  You can copy the code blocks, modify and use them on your own if you want.  I have included a final CDS object which you can access with `blaseRdata::vignette_cds`.  You can use this to make the plots and do differential gene expression if you like.  

Getting started then: 

```{r setup, results = "hide"}
# Attach the packages you will need for the analysis.
library(blaseRtools)
library(blaseRdata)
library(monocle3)
library(tidyverse)
```


Read in the analysis configuration file.  Here I am reading it from the blaseRdata package but you would substitute your own file path. 

```{r}
# Read in and inspect the configuration file.
vignette_config <- read_csv(system.file("extdata/vignette_config.csv", package = "blaseRdata"), col_type = list(.default = col_character()))
vignette_config

```


You'll notice that I added a couple of sample metadata features in addition to the required information.  We will add those to the cds.  Also notice the format of the file path.  This linux reading a windows file path.  In the config file, I used the windows "copy path" feature and simply pasted it into the csv.  The double backslashes stand in for the single backslash which is basically toxic to anything running on linux.  So this is not a problem. However, the X: stands for our network drive where the data are located.  Linux doesn't know how to access that.  So I have a helper function that will translate it for you.  This only works with the OSUMC X drive.  If you are using another drive you will have to translate that to a linux-compatible file path manually.  Here is how you use that function.  

```{r}
# Fix the windows-style file path.
vignette_config <- vignette_config %>%
  mutate(targz_path = bb_fix_file_path(targz_path))
vignette_config
```

This uses the pipe notation and tidyverse functions, which you should become familiar with if you aren't already.

Now we are ready to read in the data using the bb_load_tenx_targz function.  We will use the "apply" functional programming paradigm and the purrr package to map this function across each sample in the config file.  This will produce a list of cds objects for us.

```{r, eval = TRUE}
# Generate a list of CDS objects using purrr::map
cds_list <- map(
  .x = vignette_config$sample,
  .f = function(x, conf = vignette_config) {
    conf_filtered <- conf %>%
      filter(sample == x)
    cds <- bb_load_tenx_targz(targz_file = conf_filtered$targz_path,
                              sample_metadata_tbl = conf_filtered %>% 
                                select(-c(sample, targz_path))
                              )
    return(cds)
  }
) %>% 
  set_names(nm = vignette_config$sample)
```

Note that the line `select(-c(sample, targz_path))` removes the sample column and the file path from the sample metadata.  We don't need the file path.  As mentioned above, having "sample" in the cell metadata will cause an error later.  We use `set_names(nm = vignette_config$sample)` to name the CDS object with the sample name.  That will get added into the cell metadata later.

```{r}
# Inspect the CDS list
cds_list

# Inspect the cell metadata for the chromium_controller CDS
bb_cellmeta(cds_list$chromium_controller)
```


## QC Functions

Next we want to remove low quality cells from the analysis.  First we identify low quality cells based on a high percentage of reads mapped to mitochondrial genes or a low number of genes detected.  We will map the function bb_qc to each element of cds_list.  bb_qc itself returns a list of data objects, so we will get a list of lists.  The qc calls will be returned to the CDS objects later, but the whole output of this step is worth saving for future reference.

```{r}
ind_qc_res <- pmap(.l = list(cds = cds_list,
                             cds_name = names(cds_list),
                             genome = rep("human", times = length(cds_list))),
                   .f = bb_qc
                   ) %>%
  set_names(nm = names(cds_list))

```

For example:
```{r}
ind_qc_res$chromium_controller[3]
ind_qc_res$chromium_controller[4]
```

Next we want to remove potential cell doublets.  We identify these using a function from the *Doubletfinder* package.  This generates "pseudodoublets" and marks any real cells that map in the same area as the pseudodoublets.

First we have to figure out the anticipated doublet rate which is estimated by the number of cells in the cds/100000.

We also have to supply the qc results so we only run the prediction on high-quality cells.  


```{r}

anticipated_doublet_rate <- unlist(map(cds_list, ncol))/100000
qc_calls <- map(ind_qc_res,1)

doubletfinder_list <-
  pmap(
    .l = list(
      cds = cds_list,
      doublet_prediction = anticipated_doublet_rate,
      qc_table = qc_calls
    ),
    .f = bb_doubletfinder
  ) %>%
  set_names(names(cds_list))


```

```{r}

cds_list_rejoined <- pmap(
  .l = list(
    cds = cds_list_human,
    qc_data = purrr::map(ind_qc_res, 1),
    doubletfinder_data = doubletfinder_list
  ),
  .f = bb_rejoin
)
